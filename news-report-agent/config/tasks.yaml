# config/tasks.yaml
# ──────────────────────────────────────────────
# 🧠 각 Agent가 수행할 Task 정의
# CrewAI의 Task 객체로 변환되어 실행됨
# ──────────────────────────────────────────────

# 1️⃣ 뉴스 수집기 (Collector)
rss_scheduled_collect_task:
  description: >
    {rss_feeds}에서 최근 {hours}시간 기사를 수집해 DB에 upsert한다.
    규칙:
    - 기사 페이지 아닌 /tag|/topic|/hub|/section|/category/ 패턴은 제외
    - extract_article_content 실패/본문<300자면 scrape_tool로 폴백 후 재파싱
    - url/canonical/hash로 중복 스킵 → 임베딩 미보유 기사만 임베딩
    - 최근 {hours}시간 범위에서 클러스터 갱신 후 상위 {top_k} 이벤트로 보고서({report_format}) 생성
    - reports(scope='scheduled', source='rss', params=윈도우/피드)로 저장
  expected_output: >
    {"feeds_processed":int,"articles_collected":int,"articles_embedded":int,"events_updated":int,"report_id":int|null}
  agent: rss_collector_agent
  inputs:
    rss_feeds: []
    hours: 6
    top_k: 10
    report_format: md

serper_ad_hoc_collect_task:
  description: >
    {query}로 Serper 검색 {n_results}건 수집 → 기사만 선별 → 본문 확보 → upsert/임베딩 → 클러스터 연결/생성.
    {make_report}면 보고서({report_format}) 생성하여 reports(scope='ad_hoc', source='serper', params=질의) 저장.
  expected_output: >
    {"query":str,"articles_collected":int,"articles_embedded":int,"events_touched":int,"report_id":int|null}
  agent: serper_collector_agent
  inputs:
    query: ""
    n_results: 20
    cluster_threshold: 0.25
    make_report: true
    report_format: md
    allow_all_langs: false
agent: collector

collect_news_task:
  description: >
    RSS 피드에서 뉴스를 수집하고 데이터베이스에 저장합니다.
    
    **단계:**
    1. 지정된 RSS 피드 URL들에서 최신 기사 URL을 수집합니다.
    2. 각 URL을 방문하여 기사 본문을 스크래핑합니다.
    3. 수집된 기사들을 데이터베이스에 저장합니다.
    4. 저장 결과와 기사 ID들을 반환합니다.
  expected_output: >
    수집 및 저장 결과 요약 (가져온 기사 수, 저장된 기사 수, 임베딩 대상 ID 목록)
  agent: rss_collector_agent

# 2️⃣ 기사 임베더 (Embedder)
embed_articles_task:
  description: >
    수집된 각 기사에 대해 OpenAI 임베딩 API
    (예: text-embedding-3-small 또는 large)를 사용하여 텍스트 임베딩 벡터를 생성하고,
    해당 벡터를 PostgreSQL(pgvector) 데이터베이스에 저장합니다.

    **단계:**
    1. 각 기사에 대해 제목 + 본문 텍스트를 기반으로 임베딩을 계산합니다.
    2. `article_embeddings` 테이블에 임베딩을 삽입하거나 업데이트합니다.
    3. 저장된 임베딩 수에 대한 요약을 반환합니다.
  expected_output: >
    요약 JSON: {"num_embeddings_stored": int, "model_used": str}
  agent: embedder

# 3️⃣ 사건 클러스터링 (Clusterer)
cluster_events_task:
  description: >
    의미적으로 유사한 기사들을 사건 단위 그룹으로 클러스터링합니다.
    클러스터링 기준은 임베딩과 메타데이터(예: 시간, 키워드, 출처 등)를 기반으로 합니다.

    **단계:**
    1. 데이터베이스에서 최근 기사 임베딩을 불러옵니다.
    2. 클러스터링 수행 (예: 코사인 유사도 임계값 0.85).
    3. `events` 테이블에 사건 레코드를 생성합니다.
    4. 각 기사를 해당 사건에 연결하여 `event_articles` 테이블에 기록합니다.
  expected_output: >
    `EventList` 스키마와 일치하는 JSON 객체
  agent: clusterer

# 4️⃣ 사건 리포터 (Reporter)
generate_report_task:
  description: >
    각 사건에 대한 마크다운 형식의 요약 리포트를 생성하세요.
    다음 항목을 포함해야 합니다:
    - 사건 요약 (2~3문장)
    - 주요 주제/태그
    - 핵심 기사 출처 (제목 + 언론사)
    - 관련이 있다면 경제적 또는 지정학적 함의

    **단계:**
    1. 데이터베이스에서 사건 데이터 및 연결된 기사들을 불러옵니다.
    2. 일관성 있는 요약을 생성합니다.
    3. 결과를 `reports` 테이블에 저장합니다 (형식='md').
  expected_output: >
    `Report` 스키마와 일치하는 JSON 객체
  agent: reporter
