# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ§© í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os
import json
import time
import hashlib
from datetime import datetime

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸŒ ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import requests
import feedparser
from bs4 import BeautifulSoup
from newspaper import Article
from openai import OpenAI
from playwright.sync_api import sync_playwright

# CrewAI ê´€ë ¨
from crewai.tools import tool
from crewai_tools import SerperDevTool

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ§± ë‚´ë¶€ ëª¨ë“ˆ (í”„ë¡œì íŠ¸ ëª¨ë“ˆ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from services import db_services
from models import Article, ArticleEmbedding



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1ï¸. GoogleNewsSearchTool (Serper ê¸°ë°˜ ë‰´ìŠ¤ ê²€ìƒ‰)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# @tool("GoogleNewsSearchTool")
def search_news_with_serper(query: str) -> str:
    """
    êµ¬ê¸€ ë‰´ìŠ¤ì—ì„œ ì£¼ì œì— ë§ëŠ” ìµœì‹  ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤. 
    ì´ ë„êµ¬ëŠ” ë‰´ìŠ¤ê¸°ì‚¬ì˜ title, link, source, publishedë§Œ ë°˜í™˜í•˜ê³ , ë³¸ë¬¸ì€ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    api_key = os.getenv("SERPER_API_KEY")
    if not api_key:
        return "âŒ SERPER_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    # ì—”ë“œí¬ì¸íŠ¸ì™€ ë‹´ì•„ ë³´ë‚¼ ì •ë³´
    url = "https://google.serper.dev/news"
    headers = {"X-API-KEY": api_key}
    payload = {"q": query}

    # ìš”ì²­ë³´ë‚´ê¸°
    res = requests.post(url, headers=headers, json=payload)
    if res.status_code != 200:
        return f"âŒ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {res.status_code}"

    articles = res.json().get("news", [])
    output = []

    for article in articles:
        output.append({
            "title": article.get("title"),
            "link": article.get("link"),
            "source": article.get("source"),
            "published": article.get("date")
        })

    return json.dumps(output, indent=2, ensure_ascii=False)

# # í…ŒìŠ¤íŠ¸
# result = search_news_with_serper("Donald Trump")
# print(type(result))
# print("-" * 50)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. RSSFeedCollectorTool (RSS í”¼ë“œ ìˆ˜ì§‘ê¸°)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# @tool("RSSFeedCollectorTool")
def fetch_rss_articles(feed_url: str) -> str:
    """
    RSS í”¼ë“œë¥¼ í†µí•´ ê¸°ì‚¬ ëª©ë¡ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    feed = feedparser.parse(feed_url)
    articles = []

    for entry in feed.entries[:10]:  # ìƒìœ„ 10ê°œë§Œ
        articles.append({
            "title": entry.get("title"),
            "link": entry.get("link"),
            "published": entry.get("published", ""),
            "summary": entry.get("summary", "")
        })

    return json.dumps(articles, indent=2, ensure_ascii=False)

# í…ŒìŠ¤íŠ¸
result = fetch_rss_articles("https://feeds.bbci.co.uk/news/world/rss.xml")
print(type(result))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. scrape_tool (ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

search_tool = SerperDevTool(
    n_results=10,
)


@tool
def scrape_tool(url: str):
    """
    ì›¹ì‚¬ì´íŠ¸ì˜ ë‚´ìš©ì„ ì½ì–´ì•¼ í•  ë•Œ ì‚¬ìš©í•˜ì„¸ìš”.
    ì›¹ì‚¬ì´íŠ¸ì— ì ‘ì†í•  ìˆ˜ ì—†ëŠ” ê²½ìš° "No content"(ì½˜í…ì¸  ì—†ìŒ)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì…ë ¥ê°’ì€ url ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
    ì˜ˆì‹œ: https://www.reuters.com/world/asia-pacific/cambodia-thailand-begin-talks-malaysia-amid-fragile-ceasefire-2025-08-04/
    """

    print(f"Scrapping URL: {url}")

    with sync_playwright() as p:

        browser = p.chromium.launch(headless=True)

        page = browser.new_page()

        page.goto(url)

        time.sleep(5)

        html = page.content()

        browser.close()

        soup = BeautifulSoup(html, "html.parser")

        unwanted_tags = [
            "header",
            "footer",
            "nav",
            "aside",
            "script",
            "style",
            "noscript",
            "iframe",
            "form",
            "button",
            "input",
            "select",
            "textarea",
            "img",
            "svg",
            "canvas",
            "audio",
            "video",
            "embed",
            "object",
        ]

        for tag in soup.find_all(unwanted_tags):
            tag.decompose()

        content = soup.get_text(separator=" ")

        return content if content != "" else "No content"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. CanonicalScraperTool (ê¸°ì‚¬ ë³¸ë¬¸ + ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸°)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@tool("CanonicalScraperTool")
def extract_article_content(url: str) -> str:
    """
    ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    try:
        article = Article(url)
        article.download()
        article.parse()

        result = {
            "title": article.title,
            "authors": article.authors,
            "published_date": str(article.publish_date) if article.publish_date else None,
            "text": article.text,
            "url": url
        }

        return json.dumps(result, indent=2, ensure_ascii=False)

    except Exception as e:
        return f"âŒ ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"







# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # 0ï¸âƒ£ ê¸°ë³¸ ì„¤ì •
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# client = OpenAI(api_key=OPENAI_API_KEY)
# 
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # 1ï¸âƒ£ ì›¹ ê¸°ì‚¬ ìŠ¤í¬ë˜í•‘ ë„êµ¬
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# @tool
# def scrape_article(url: str) -> str:
#     """
#     ì›¹í˜ì´ì§€ì˜ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•œë‹¤.
#     - ê´‘ê³ , ë©”ë‰´, ëŒ“ê¸€ ë“±ì„ ì œì™¸í•˜ê³  <p> íƒœê·¸ ì¤‘ì‹¬ìœ¼ë¡œ ë³¸ë¬¸ë§Œ ëª¨ì€ë‹¤.
#     - ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´ì„ ë°˜í™˜í•œë‹¤.
#     """
#     try:
#         res = requests.get(url, timeout=10)
#         res.raise_for_status()
#         soup = BeautifulSoup(res.text, "html.parser")

#         paragraphs = [p.get_text(strip=True) for p in soup.find_all("p")]
#         body = "\n".join(p for p in paragraphs if len(p) > 50)  # ì§§ì€ ë¬¸ì¥ì€ ì œì™¸
#         return body
#     except Exception as e:
#         print(f"[scrape_article] Error fetching {url}: {e}")
#         return ""


# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # 2ï¸âƒ£ ì„ë² ë”© ìƒì„± ë„êµ¬
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# @tool
# def generate_embedding(article_id: int, text: str, model: str = "text-embedding-3-small"):
#     """
#     ê¸°ì‚¬ ë³¸ë¬¸ì„ OpenAI Embeddingìœ¼ë¡œ ë³€í™˜í•˜ê³  DBì— ì €ì¥í•œë‹¤.
#     """
#     try:
#         response = client.embeddings.create(model=model, input=text)
#         vector = response.data[0].embedding

#         emb = ArticleEmbedding(article_id=article_id, model=model, dim=len(vector), embedding=vector)
#         db_service.upsert_embedding(emb)

#         return {"article_id": article_id, "embedding_dim": len(vector), "model": model}
#     except Exception as e:
#         print(f"[generate_embedding] Error: {e}")
#         return {"error": str(e)}


# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # 3ï¸âƒ£ ë‰´ìŠ¤ ê¸°ì‚¬ ì €ì¥ ë„êµ¬
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# @tool
# def save_article(outlet_name: str, outlet_domain: str, country_code: str, title: str, url: str, body: str, published_at: str = None, language: str = "en"):
#     """
#     ê¸°ì‚¬ ì •ë³´ë¥¼ DBì— ì €ì¥í•œë‹¤.
#     - outletì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
#     - articleì€ url ì¤‘ë³µ ì‹œ upsert
#     """
#     try:
#         outlet_id = db_service.upsert_outlet(
#             Article.Outlet(name=outlet_name, domain=outlet_domain, country_code=country_code)
#         ) if hasattr(Article, "Outlet") else db_service.upsert_outlet(
#             type("Outlet", (), {"name": outlet_name, "domain": outlet_domain, "country_code": country_code})()
#         )

#         hash_sha256 = hashlib.sha256((title + url).encode()).hexdigest()
#         article = Article(
#             outlet_id=outlet_id,
#             url=url,
#             title=title,
#             body=body,
#             language=language,
#             published_at=datetime.fromisoformat(published_at) if published_at else None,
#             hash_sha256=hash_sha256,
#         )

#         inserted_ids = db_service.upsert_articles([article])
#         return {"inserted_id": inserted_ids[0], "url": url}
#     except Exception as e:
#         print(f"[save_article] Error saving article: {e}")
#         return {"error": str(e)}


# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # 4ï¸âƒ£ ê¸°ì‚¬ ìœ ì‚¬ë„ ê²€ìƒ‰ (ì„ íƒ)
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# @tool
# def search_similar_articles(query_text: str, top_k: int = 5, model: str = "text-embedding-3-small"):
#     """
#     ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©í•˜ê³ , DB ë‚´ ê¸°ì‚¬ë“¤ê³¼ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ ìˆ˜í–‰.
#     (pgvector cosine similarity)
#     """
#     try:
#         response = client.embeddings.create(model=model, input=query_text)
#         vector = response.data[0].embedding

#         sql = """
#         SELECT a.id, a.title, a.url, 1 - (aemb.embedding <=> %s::vector) AS similarity
#         FROM article_embeddings aemb
#         JOIN articles a ON a.id = aemb.article_id
#         ORDER BY aemb.embedding <=> %s::vector
#         LIMIT %s;
#         """

#         with db_service.get_conn() as conn, conn.cursor() as cur:
#             cur.execute(sql, (vector, vector, top_k))
#             rows = cur.fetchall()

#         return [{"id": r[0], "title": r[1], "url": r[2], "similarity": float(r[3])} for r in rows]
#     except Exception as e:
#         print(f"[search_similar_articles] Error: {e}")
#         return {"error": str(e)}
