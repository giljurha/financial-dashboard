# tests/smoke_orchestrate.py
import json
from services.orchestrator import fetch_scrape_upsert

# â¬‡ï¸ ì¶”ê°€: ë¶€ë¶„ ì»¤ë°‹ìš©ìœ¼ë¡œ ê¸°ì‚¬ ì›ë³¸ì„ ì§ì ‘ ê°€ì ¸ì™€ì„œ 1~3ê°œë§Œ ì €ì¥
from services.fetch import fetch_scrape, extract_firecrawl
from services.persist import persist_articles

# RSS í”¼ë“œ ëª©ë¡ â€” ì „ ì„¸ê³„ ì£¼ìš” ë‰´ìŠ¤
RSS_FEEDS = [
    "https://feeds.bbci.co.uk/news/rss.xml",
    # "https://feeds.feedburner.com/dailycaller",
    # "https://www.theepochtimes.com/us/us-politics/feed?_gl=1%2Ak929d7%2A_gcl_au%2AMTcyMDM2MDcyMy4xNzYwMTYwMzU1",
    "https://www.newsmax.com/rss/Newsfront/16/",
    "https://www.france24.com/en/rss",
    # "https://www.aljazeera.com/xml/rss/all.xml",
    "https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml",
    # "https://rss.dw.com/rdf/rss-en-all",
    # "https://moxie.foxnews.com/google-publisher/latest.xml",
]

# ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì™€ ë™ì¼í•œ ìµœì†Œ í’ˆì§ˆ ê°€ë“œ(ë³¸ë¬¸ ê¸¸ì´ ê¸°ì¤€)
def good(a: dict) -> bool:
    # ì œëª©/URL í•„ìˆ˜, ë³¸ë¬¸ì€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ í—ˆìš©(ì§§ì•„ë„ OK)
    if not a.get("url") or not a.get("title"):
        return False
    txt = (a.get("text") or "").strip()
    return len(txt) > 300


if __name__ == "__main__":
    # â‘  ë“œë¼ì´ëŸ° ëª¨ë“œ (DB ì €ì¥ ì•ˆ í•¨) â€” ì „ì²´ íŒŒì´í”„ë¼ì¸ ë©”íŠ¸ë¦­ í™•ì¸
    result_dry = fetch_scrape_upsert(
        rss_feeds=RSS_FEEDS,
        batch_limit=10_000,
        commit=False
    )
    print("ğŸ§ª [Dry Run]")
    print(json.dumps(result_dry, ensure_ascii=False, indent=2))

    # â‘¡ ë¶€ë¶„ ì»¤ë°‹: good í•¨ìˆ˜ë¥¼ í†µê³¼í•œ ê¸°ì‚¬ë“¤ë§Œ ì €ì¥
    print("\nğŸ’¾ [Full Commit] í’ˆì§ˆ í•„í„°ë§ëœ ê¸°ì‚¬ ì €ì¥ í…ŒìŠ¤íŠ¸")
    fetched_json = fetch_scrape(RSS_FEEDS)       # ê¸°ì‚¬ ì›ë³¸ ë¦¬ìŠ¤íŠ¸ ì§ì ‘ í™•ë³´
    fetched = json.loads(fetched_json)
    all_articles = fetched.get("articles", [])
    
    # good í•¨ìˆ˜ë¥¼ í†µê³¼í•œ ê¸°ì‚¬ë“¤ë§Œ í•„í„°ë§
    articles = [a for a in all_articles if good(a)]
    print(f"ğŸ“Š ì „ì²´ ê¸°ì‚¬: {len(all_articles)}ê°œ â†’ í’ˆì§ˆ í•„í„°ë§ í›„: {len(articles)}ê°œ")


    if not articles:
        print("âš ï¸ ì»¤ë°‹í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        result_commit = persist_articles(articles)
        report = {
            "total_size": len(articles),
            **result_commit,   # processed/inserted/updated/skipped/errors
        }
        print(json.dumps(report, ensure_ascii=False, indent=2))

        print("\nâœ… ì €ì¥ ì‹œë„í•œ íƒ€ì´í‹€:")
        for a in articles:
            print("   â€¢", a.get("title"))

# if __name__ == "__main__":
#     for url in RSS_FEEDS:
#         print(extract_firecrawl(url))
#         print("-" * 100)
